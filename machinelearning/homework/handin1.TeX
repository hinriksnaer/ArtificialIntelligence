\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{multicol}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{Machine Learning I}
\newcommand\hwnumber{1}                  % <-- homework number
\newcommand\NetIDa{Hinrik Snær Guðmundsson}           % <-- NetID of person #1
\newcommand\NetIDb{12675326}           % <-- NetID of person #2 (Comment this line out for problem sets)

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa\\\NetIDb}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course \\ \today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\section*{2 Multivariable Calculus}
\subsection*{Question 2.1}

The following questions are good practice in manipulating vectors and matrices.
Compute the following gradients, assuming $\boldsymbol{\Sigma}^{-1}$
is symmetric, positive semidefinite and invertible. Simplify your answers as much as possible.

\begin{enumerate}[(a)]
    \item 
    $\nabla_{\boldsymbol{\mu}}(\boldsymbol{x}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})$
    
    Answer:
    
    \begin{flalign*}
        \nabla_{\boldsymbol{\mu}}(\boldsymbol{x}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu}) =& \nabla_{\boldsymbol{\mu}} (\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1} - \boldsymbol{\mu}^T\boldsymbol{\Sigma}^{-1})(\boldsymbol{x}-\boldsymbol{\mu}) \\
        =& \nabla_{\boldsymbol{\mu}}\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} -  \boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu} - \boldsymbol{\mu}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} + \boldsymbol{\mu}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}
        \\
        =& -\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\Sigma}^{-1}\boldsymbol{x} + 2\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu} 
        \\
    =& 2\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}-\boldsymbol{x})
    \end{flalign*}
    
    \item
    $\nabla_{\boldsymbol{q}} - \boldsymbol{p}^{T}log(\boldsymbol{q})$, where log(.) is applied element wise.
    
    Answer: 
    \begin{flalign*}
\nabla_{\boldsymbol{q}} - \boldsymbol{p}^{T}log(\boldsymbol{q}) =& -\nabla_{\boldsymbol{q}} \sum_{i=1}^{N} p_i log(q_i)\\
    =& - \begin{bmatrix} \frac{p_1}{q_1} \dots \frac{p_n}{q_n}\end{bmatrix}
    \end{flalign*}
    \item
    $\nabla_{\boldsymbol{Wf}}$, where $\boldsymbol{f} = \boldsymbol{Wx}, \boldsymbol{W}\in \mathbb{R}^{2\times3}$, and $\boldsymbol{x} \in \mathbb{R}^3$. Follow Example 5.11 of the book mathematics for machine learning to solve this
    
    Answer:
    We start by determining the dimension of the gradient as
    
    \begin{align*}
        \frac{d \boldsymbol{f}}{d \boldsymbol{A}} \in \mathbb{R}^{M \times (M \times N)} \implies \frac{d \boldsymbol{f}}{d \boldsymbol{A}} \in \mathbb{R}^{2 \times (2 \times 3)}
    \end{align*}
    
    The gradient will be the collection of the partial derivatives
    
    \begin{align*}
        \frac{d\boldsymbol{f}}{d \boldsymbol{W}} = \begin{bmatrix} \frac{\partial f_1}{\partial \boldsymbol{W}} \\ \\ \frac{\partial f_2}{\partial \boldsymbol{A}}\end{bmatrix}
    \end{align*}
    
    \begin{align*}
        f_i = \sum_{j=1}^{3} W_{ij}x_j \qquad i = 1, 2
    \end{align*}
    
    and the partial derivative is
    
    \begin{align*}
        \frac{\partial f_i}{\partial W_{iq}} = x_q
    \end{align*}
    
    We compute the partial derivatives of $f_i$ with respect to a row of \boldsymbol{W}
    which is given as 
    
    \begin{align*}
        \frac{\partial f_i}{\partial W_{i,:}} = \boldsymbol{x}^T \in \mathbb{R}^{1 \times (1 \times 3)}
    \end{align*}
    
    \item 
    $\nabla_{\boldsymbol{W}f}$, where $f = (\boldsymbol{\mu} - \boldsymbol{Wx})$ where $\boldsymbol{W} \in \mathbb{R}^{M\timesK}$
    
\end{enumerate}


\section*{3 Probability theory}

\subsection*{Question 3.1}

A little warmup. This question is based on one of the early chapters in Probability Theory: The Logic of Science by E.T. Jaynes. Consider the following setting. You are driving down the street at night and suddenly you see a man climbing through a broken window of a jewelry store. Then, he runs away carrying a bag over his shoulder. For many of us, our gut reaction would be to think the man in question is a criminal. Why do we draw this conclusion instead of another scenario? Let’s explore this using the methods of Probability Theory.

\begin{enumerate}[(a)]
  \item
    Explain in words: why would many people draw the conclusion that the man in question is a criminal? Try to think in terms of probability. [1–3 sentences is sufficient] 
   
    Answer: If we base our gut reaction on previous knowledge gained from similar circumstances through various scenarios that are similar in nature, it is clear that the amount of times the person was a criminal far exceeds the amount of times the person has not been a criminal. Given that information we can deduct that the likelihood of the person being a criminal far exceeds the likelihood that he isn't. Because of this we see that the expected result is him being a criminal and that is the conclusion our gut reaction jumps towards.

  \item
    Show, formally, that the probability of us believing the man is a criminal
    given our observation is based on our beliefs of making this observation
    when the man is a criminal and making the observation when the man
    is not a criminal.
    
    Let’s assume one in every $10^5$ people is in fact a criminal, the probability
    of making this observation when the man is not a criminal is $\frac{1}{10^6}$ , and
    that of making this observation when the man is a criminal is 0.8.
    
    
    Answer: 
    
    Let X denote men and Y denote our assumption on whether or not a man is a criminal. We are given that on in every ${10^5}$ men are criminals, based on that we can derive that
    \begin{multicols}{2}
      \begin{equation*}
        P(X = Criminal) = \frac{1}{10^5}
      \end{equation*}\break
      \begin{equation*}
        P(X = Innocent) = 1 - \frac{1}{10^5}
      \end{equation*}
    \end{multicols}
    
    We are also given that when we believe someone is a criminal, there is a $\frac{1}{10^6}$ chance that the man is actually innocent and when we accuse someone of being a criminal, there is a 0.8 chance that we are correct. Based on that we can derive the following formulas
    
    \begin{multicols}{2}
      \begin{equation*}
        P(Y = Criminal | X = Innocent) = \frac{1}{10^6}
      \end{equation*}\break
      \begin{equation*}
        P(Y = Criminal | X = Criminal) = 0.8
      \end{equation*}
    \end{multicols}
    
    which allows us to also derive the following
    
    \begin{multicols}{2}
      \begin{equation*}
        P(Y = Innocent | X = Innocent) = 1 - \frac{1}{10^6}
      \end{equation*}\break
      \begin{equation*}
        P(Y = Innocent | X = Criminal) = 1 - 0.8
      \end{equation*}
    \end{multicols}
    
    We can use the sum rule to calculate the probability of a person being assumed to be a criminal.
    \begin{flalign*}
    P(Y = Criminal) =& P(Y = C|X = C)P(X = C) + P(Y = C | X = I)P(X = I) \\
    =& 0.8\times\frac{1}{10^5} + \frac{1}{10^6}\times(1 - \frac{1}{10^5}) \\
    \approx& 9.0\times10^{-6}
    \end{flalign*}
    
    And the probability of a person being assumed innocent is 
    
     \begin{flalign*}
    P(Y = Innocent) \approx 1 - 9.0\times10^{-6}
    \end{flalign*}
    
  \item
    Compute the probability of the man being a criminal based on our observations.
    
    Answer:
    
    The equation we are asked to solve is the following
    \begin{flalign*}
    P(X = Criminal) =& P(X = C | Y = C)P(Y = C) + P(X = C | Y = I)P(Y = I) \\
    \end{flalign*}
    
    First we can use Bayes to find the following
    
    \begin{multicols}{2}
      \begin{equation*}
        P(X = C|Y = C)
      \end{equation*}\break
      \begin{equation*}
        P(X = C|Y = I)
      \end{equation*}
    \end{multicols}
    
    \begin{align*}
    	P(X = C|Y = C) &= \frac{P(Y = C|X = C)P(X=C)}{P(Y=C)} \\
    	&= \frac{0.8\times\frac{1}{10^{5}}}{9.0\times10^{-6}}
    	\\
    	&\approx 0.889 
    \end{align*}
    
    \begin{align*}
    	P(X = C|Y = I) &= \frac{P(Y = I|X = C)P(X=C)}{P(Y=I)} \\
    	&= \frac{0.2\times\frac{1}{10^{5}}}{1 - 9.0\times10^{-6}}
    	\\
    	&\approx 2\times10^{-6}
    \end{align*}
    
    Now we substitute the values into the equation and calculate the probability of a person being a criminal.
    
    \begin{align*}
    	P(X = Criminal) =& P(X = C | Y = C)P(Y = C) + P(X = C | Y = I)P(Y = I) \\
    	&= 0.889\times9.0\times10^{-6} + 2\times10^{-6}\times(1 - 9.0\times10^{-6})\\
    	&\approx 1\times10^{-5}
    \end{align*}
    
\end{enumerate}
\subsection*{Question 3.2}

\begin{enumerate}[(a)]
  \item    
    We are asked to write down the expression for the likelihood of the observed datapoints.
    \begin{align*}
p(\mathcal{D}|\boldsymbol{\rho}) = \prod_{k=1}^{K} \rho_{k}^{m_k}
    \end{align*}
    where 
    \begin{align*}
        m_k = \sum_{n} x_{nk}
    \end{align*}
    
    \item
        Based on our observed data we can compute $\rho_i$ as
        \begin{align*}
            \rho_i = \frac{1}{N} \sum_{j = 1}^N x_{ji}
        \end{align*}
        which gives us
        \begin{align*}
            \boldsymbol{\rho} = \begin{bmatrix} \frac{1}{5} & 0 & 0 & \frac{1}{5}\end{bmatrix}^T
        \end{align*}
        \item
        We user Bernoulli distribution to describe the probability of drawing a red card.
        
        The likelihood function for the Bernoulli distribution is given by
        \begin{align*}
            p(\mathcal{D}|\mu) =  \prod_{n=1}^{N} \mu^{x_n}(1-\mu)^{1-x_n}
        \end{align*}
        We can rewrite the expression so that p denotes the probability of a red card being drawn and $x_{n1}$ and $x_{n3}$ being a red card as
        \begin{align*}
            p(\mathcal{D}|p) =  \prod_{n=1}^{N} p^{x_{n1}+x_{n3}}(1-p)^{1-x_{n1} + x_{n3}}
        \end{align*}
        
        and the probability of a card being red is written as
        
        \begin{align*}
            p = \frac{1}{N} \sum_{j = 1}^N x_{j1} + x_{j3}
        \end{align*}
    
    \item
        Using the formula we defined to evaluate $p$ we can compute the values of $p$ that most likely generate these observations
        \begin{align*}
             p = \frac{1}{N} \sum_{j = 1}^N x_{j1} + x_{j3} =  \frac{6}{8} = \frac{3}{4}
        \end{align*}

    \item 
        $p$ can be computed by taking the sum of all $\rho_i$ where $\rho_i$ is a red card, in our specific case it can be represented as
        
        \begin{align*}
            p = \rho_1 + \rho_3
        \end{align*}
    
    \item 
        \begin{align*}
            p(p | \mathcal{D}) = \frac{p(\mathcal{D}|p)p(p)}{p(\mathcal{D)}}
        \end{align*}
        
        Where 
        
        $p(p|\mathcal{D})$ represents the posterior
        
        $p(\mathcal{D}|p)$ represents the likelihood

        $p(p)$ represents the prior
        
        $p(\mathcal{D})$ represents the evidence
        
        
    \item
                
        
\end{enumerate}
\end{document}
